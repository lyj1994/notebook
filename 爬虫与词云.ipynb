{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "爬虫与词云.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1IXqvVPSZP_DpLkzet_clleTclJvmjPGl",
      "authorship_tag": "ABX9TyNRYi6kPhCY816HHCZaEPoT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lyj1994/notebook/blob/main/%E7%88%AC%E8%99%AB%E4%B8%8E%E8%AF%8D%E4%BA%91.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYUl_oCBCy_5"
      },
      "source": [
        "## 背景\n",
        "\n",
        "这次想要做的是把《经济研究》的题目都爬取出来，然后通过中文分词后，生成一个词云图。  \n",
        "把大象关进冰箱分三步，我们这个也只需要3步。  \n",
        "1. 爬取数据\n",
        "2. 分词\n",
        "3. 生成词云\n",
        "\n",
        "## 爬取数据\n",
        "\n",
        "先说爬取数据，直接找到知网的经济研究详情页：https://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&pykm=JJYJ&Year=&Issue=  \n",
        "打开Chrome的控制台，发现每次切换左侧的刊登月份时，都会发一个请求，形如：https://navi.cnki.net/knavi/JournalDetail/GetArticleList?year=2020&issue=08&pykm=JJYJ&pageIdx=0&pcode=CJFD ，很明显，URL中的`year`和`issue`用来控制刊物的年份和月份，`pykm`就是对应的刊物了，`pageIdx`则是用来分页的，我们先来爬一个2020年8月的刊物。  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yf_S5cHmZHcB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f00e49a-3bbc-4851-eef1-69686676032f"
      },
      "source": [
        "# 安装需要的库和依赖\n",
        "!pip install requests\n",
        "!pip install beautifulsoup4\n",
        "!pip install jieba\n",
        "!pip install urllib2\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "response = requests.get('https://navi.cnki.net/knavi/JournalDetail/GetArticleList?year=2020&issue=08&pykm=JJYJ&pageIdx=0&pcode=CJFD')\n",
        "response.text\n",
        "\n",
        "text = response.content.decode('utf8')\n",
        "soup = BeautifulSoup(text, 'html.parser')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests) (3.0.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (4.6.3)\n",
            "Requirement already satisfied: jieba in /usr/local/lib/python3.7/dist-packages (0.42.1)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement urllib2 (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for urllib2\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ES1RwQ78GDmh"
      },
      "source": [
        "## 处理数据&分词\n",
        "我们观察到，这个返回的HTML脚本里面，使用下面的结果包裹了每一篇的标题和链接：\n",
        "\n",
        "```html\n",
        "<span class=\"name\">\n",
        "    <a target=\"_blank\" href=\"Common/RedirectPage?sfield=FN&amp;dbCode=CJFD&amp;filename=JJYJ202008002&amp;tableName=CJFDAUTO&amp;url=\">\n",
        "        全球经济大变局、中国潜在增长率与后疫情时期高质量发展\n",
        "    </a>\n",
        "</span>\n",
        "```\n",
        "\n",
        "所以通过`BeautifulSoup`处理过后的脚本，我们可以通过find_all方法找到对应的span标签，然后，在for循环中，一一取出对应的href和text。\n",
        "在for循环处理时，也可以顺便使用[`jieba`分词](https://github.com/fxsjy/jieba)把标题都分析出来，最后拼接成一个长字符串，用来之后生成词云。  \n",
        "当然，只有一期刊物是不行的。We need more！  \n",
        "\n",
        "## 多个刊物\n",
        "\n",
        "如果有什么问题一次循环解决不了，那就两次。  \n",
        "我们爬取2020年8月，传入的year是2020，issue是08，所以，显然，把这里year和issue变更一下就好了。\n",
        "代码如下："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1N6V0lGTpLfx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        },
        "outputId": "a8b7ee42-7542-4a21-d0e4-51831e088c46"
      },
      "source": [
        "from urllib.parse import parse_qs\n",
        "import jieba\n",
        "import jieba.analyse\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "\n",
        "start_year = 2020\n",
        "stop_year = 2018 # 只爬千禧年之后的数据\n",
        "issue_list = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12']\n",
        "\n",
        "# 这里的排除词主要是排除一些常用但是对我们价值不大的词汇\n",
        "exclude_word_list = ['基于', '综述', '启事', '学院', '教师', '笔谈', '来自', '一个', '公告', '主编', '经济', '中国'] \n",
        "\n",
        "essay_notice = '征文启事' # 去除征文启事这种无关标题\n",
        "prefix = 'https://kns.cnki.net/kcms/detail/detail.aspx'\n",
        "\n",
        "jieba.enable_paddle()\n",
        "\n",
        "title_href_list = [] # 存放检索结果的list\n",
        "participles = '' # 记录全部分词的长字符串\n",
        "\n",
        "def participles_generator (year, issue, o_participles):\n",
        "    t_participles = o_participles\n",
        "    response = requests.get('https://navi.cnki.net/knavi/JournalDetail/GetArticleList?year=' + str(year) + '&issue='+ issue +'&pykm=JJYJ&pageIdx=0&pcode=CJFD')\n",
        "    text = response.content.decode('utf8')\n",
        "    soup = BeautifulSoup(text, 'html.parser')\n",
        "    span_name_list = soup.find_all(\"span\", \"a\", class_=\"name\")\n",
        "\n",
        "    for tag in span_name_list:\n",
        "        title = tag.find('a').text.strip() # 去掉空白字符的影响\n",
        "        if (title.find(essay_notice) == -1): # 去掉征文启事\n",
        "            query = parse_qs(prefix + tag.find('a')['href'])\n",
        "            # 把标题和链接先保存起来，词云生成暂不使用\n",
        "            title_href_list.append([title, prefix + '?dbname' + query['dbCode'][0] + '&filename=' + query['filename'][0] + '&dbname=' + query['tableName'][0], jieba.analyse.extract_tags(title, withWeight=True)])\n",
        "            # 把分词结果写入到一个长字符串里\n",
        "            for x, w in jieba.analyse.extract_tags(title, withWeight=True):\n",
        "                if (w > 0.5 and exclude_word_list.count(x) == 0): # 权重大于0.5才使用，去掉一些水词\n",
        "                    t_participles = t_participles + ' ' + x\n",
        "    return t_participles\n",
        "\n",
        "while (start_year > stop_year):\n",
        "    for idx, issue in enumerate(issue_list):\n",
        "        if (start_year == 2020 and idx >= 8): # 截止编写代码之时，2020只有到8月刊\n",
        "            pass\n",
        "        else:\n",
        "            print('当前执行月份：{}年{}月'.format(start_year, issue))\n",
        "            time.sleep(5) # 每次睡5s，防止爬取太多频次被封\n",
        "            participles = participles_generator(start_year, issue, participles)\n",
        "    start_year = start_year - 1\n",
        "participles\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Installing paddle-tiny, please wait a minute......\n",
            "Paddle enabled successfully......\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "当前执行月份：2020年01月\n",
            "当前执行月份：2020年02月\n",
            "当前执行月份：2020年03月\n",
            "当前执行月份：2020年04月\n",
            "当前执行月份：2020年05月\n",
            "当前执行月份：2020年06月\n",
            "当前执行月份：2020年07月\n",
            "当前执行月份：2020年08月\n",
            "当前执行月份：2019年01月\n",
            "当前执行月份：2019年02月\n",
            "当前执行月份：2019年03月\n",
            "当前执行月份：2019年04月\n",
            "当前执行月份：2019年05月\n",
            "当前执行月份：2019年06月\n",
            "当前执行月份：2019年07月\n",
            "当前执行月份：2019年08月\n",
            "当前执行月份：2019年09月\n",
            "当前执行月份：2019年10月\n",
            "当前执行月份：2019年11月\n",
            "当前执行月份：2019年12月\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "''"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRdgrJfxBUdw"
      },
      "source": [
        "## 生成词云\n",
        "在处理词云数据之前，需要下载中文字体，因为一些字体收费或者不能商用等原因，选择了思源黑体，大家可以在 GitHub 上下载下来，使用 regular.ttf 即可。https://github.com/Pal3love/Source-Han-TrueType。  \n",
        "类似于本地加载字体文件，在 Colab 中加载文件也需要显式的文件地址，但是 Colab 采用的容器，每次重新启动后，上一次传入文件就会销毁。所以我们需要上传到账号对应的 Google 网盘，然后在 Colab 的笔记本中加载 Google 网盘中文件，找到对应的文件以后，点击三个点，选择“复制路径”，就可以使用 Image.open 打开这个路径了。  \n",
        "需要注意的是，我这里写的是我自己的路径，如果大家可以根据自己实际网盘路径吧这两个path都修改一下。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTgNIPmb7vaC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        },
        "outputId": "2ba521b0-e00d-4ee0-e8d4-f7bc397790ba"
      },
      "source": [
        "from wordcloud import WordCloud\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import requests\n",
        "from io import BytesIO\n",
        "\n",
        "# 这里换成了一张网络图片地址，也可以直接使用 Image.open('/image_path') 打开本地图片\n",
        "response = requests.get(\"https://i.loli.net/2020/10/26/2eiPWczMoRgrxEb.png\")\n",
        "img = Image.open(BytesIO(response.content))\n",
        "\n",
        "map_mask = np.array(img)\n",
        "\n",
        "# 自行找到ttf字体文件存放的目录，然后拷贝过来\n",
        "wordcloud = WordCloud(font_path=\"/content/drive/My Drive/ttf/SourceHanSansCN-Regular.ttf\",background_color=\"white\", max_words=2000, mask=map_mask, width=2400, height=1800, max_font_size=100).generate(participles)\n",
        "%pylab inline\n",
        "plt.figure(dpi=200)\n",
        "plt.imshow(wordcloud, interpolation='bilinear',)\n",
        "plt.axis(\"off\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-ff43c6dc997b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# 自行找到ttf字体文件存放的目录，然后拷贝过来\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mwordcloud\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfont_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/content/drive/My Drive/ttf/SourceHanSansCN-Regular.ttf\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbackground_color\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"white\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmap_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2400\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1800\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_font_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparticiples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pylab inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdpi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/wordcloud/wordcloud.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    603\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m         \"\"\"\n\u001b[0;32m--> 605\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_generated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/wordcloud/wordcloud.py\u001b[0m in \u001b[0;36mgenerate_from_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    585\u001b[0m         \"\"\"\n\u001b[1;32m    586\u001b[0m         \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 587\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_from_frequencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    588\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/wordcloud/wordcloud.py\u001b[0m in \u001b[0;36mgenerate_from_frequencies\u001b[0;34m(self, frequencies, max_font_size)\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrequencies\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m             raise ValueError(\"We need at least 1 word to plot a word cloud, \"\n\u001b[0;32m--> 383\u001b[0;31m                              \"got %d.\" % len(frequencies))\n\u001b[0m\u001b[1;32m    384\u001b[0m         \u001b[0mfrequencies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfrequencies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_words\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: We need at least 1 word to plot a word cloud, got 0."
          ]
        }
      ]
    }
  ]
}